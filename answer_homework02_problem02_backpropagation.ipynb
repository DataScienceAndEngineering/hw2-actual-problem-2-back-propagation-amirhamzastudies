{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"ANSWER: 1. To prove the equation\n\n(\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j))\n\nWe will follow a similar approach as in the previous proof, but with notation adjusted to match the notation you provided.\n\n* Error in Layer l(b^l_j):\nIn layer l,  the error term for the j-th neuron, delta^l_j, is computed as:\n\n\\delta^l_j = \\frac{\\partial C}{\\partial z ^l_j}.....(1)\n\nwhere z^l_j is the weighted input to neuron j in layer l.\n\n* Partial Derivative of the Cost Function with respect to Bias (\\frac{\\partial C}{\\partial b^l_j}):\nThe bias term b^l_j contributes directly to the weighted input z^l_j for neuron j in layer l. Thus, the partial derivative of the cost function C with respect to the bias term b^l_j in layer l is:\n\n(\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}).....(2)\n\nEquating the Error Term and the Partial Derivative of the Cost Function:\nFrom euation 1 and 2, we can see that,\n\n (\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j)\nthus, the equation holds.\n\nTherefore, the equation (\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j) is proven, showing that the partial derivative of the cost function with respect to the bias term in layer l is indeed equal to the term for that neuron.\nThis is a fundamental outcome in backpropagation, where the error signals are used to update the biases during training.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANSWER: 2. To prove the equation\n[\\frac{\\partial C}{\\partial w^{l}{jk}} = a^{l-1}{k} \\ delta^{l}_{j}]\n\nwhere \\frac{\\partial C}{\\partial w^{l}{jk}}  represents the partial derivative of the cost function C with respect to the weight w^{l}{jk}, connecting the k-th neuron in layer l-1 to the j-th neuron in layer l, and delta^l_j demotes the error term of neuron j in layer l, lets's outline the proof:\n\n* In terms of error, delta^l_j represents the derivative of the cost function with respect to the weighted input of neuron j in layer l.  It is typically computed during the backpropagation process.\n\n* For partial derivative of the cost function with respect to the weight, \\frac{\\partial C}{\\partial w^{l}{jk}}, we need to express the partial derivative of the cost function with respect to the weight, w^{l}{jk} in terms of the error term delta^l_j and the activation of neuron k in the previous layer l-1, denoted by a^{l-1}{k}.\n\n* By the chain rule, we can express the partial derivative of the cost function with respect to the weight, w^{l}{jk} as the product of the error term delta^{l}_{j} and the activation a^{l-1}{k}.\n\n    [\\frac{\\partial C}{\\partial w^{l}{jk}} = delta^{l}_{j} \\  a^{l-1}{k}]\n\nThis the equation is proved.\n    [\\frac{\\partial C}{\\partial w^{l}{jk}} = a^{l-1}{k} \\ delta^{l}_{j}]\n\nThis shows that the partial derivative of the cost function with respect to the weight, w^{l}{jk} is indeed equal to the product of the activation of the neuron in the previous layer and the error term of the neuron in the current layer, as expected in backpropagation. So, the equation is proven.\n","metadata":{},"execution_count":null,"outputs":[]}]}